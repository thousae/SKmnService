{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy\n",
    "!pip install lexrankr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "k2Zl7CL0RUim"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#기사1\n",
    "sents = '''\n",
    "삼성미래기술육성사업이 반가운 소식을 전했습니다.\n",
    "울산과학기술원(UNIST) 에너지 및 화학공학부 이준희 교수 연구팀이 차세대 메모리 반도체의 집적도를 1,000배 이상 향상할 수 있는 이론과 소재를 발표했다고 삼성미래기술육성사업이 전했습니다.\n",
    "이 연구는 현지시간으로 오늘(2일) 세계적인 학술지 사이언스(Science)에 게재됐다. 사이언스에 순수 이론 논문이 게재되는 경우는 극히 드문 사례로, 국내 연구팀 단독 교신으로 진행한 이 연구는 이론적 엄밀성과 독창성, 산업적 파급력을 인정받아 게재됐습니다.\n",
    "반도체 업계는 소자의 성능을 향상하기 위해 미세화를 통해 단위 면적당 집적도를 높여왔습니다.\n",
    "하지만 데이터 저장을 위해서는 탄성으로 연결된 수천 개의 원자 집단인 `도메인`이 필요해 일정 수준 이하로 크기를 줄일 수 없는 제약사항이 있었습니다.\n",
    "반도체 소자가 한계 수준 이하로 작아지면 정보를 저장하는 능력이 사라지는 `스케일링(Scaling)` 이슈현상이 발생하기 때문입니다.\n",
    "이렇게 되면 반도체의 기본 작동원리인 0과 1을 제대로 구현할 수 없습니다.\n",
    "이준희 교수 연구팀은 `산화하프늄(HfO₂)`이라는 반도체 소재의 산소 원자에 전압을 가하면 원자간 탄성이 사라지는 물리 현상을 새롭게 발견하고, 반도체에 적용해 저장 용량 한계를 돌파하는 데 성공했습니다.\n",
    "이 현상을 적용하면 개별 원자를 제어할 수 있고 산소 원자 4개에 데이터(1bit) 저장이 가능해져, 데이터 저장을 위해 수십 nm(나노미터) 크기의 도메인이 필요하다는 업계 통념을 뒤집었습니다.\n",
    "산화하프늄은 현재 메모리 반도체 공정에서 흔히 사용하는 소재로, 이 현상을 적용할 경우 스마트폰, 태블릿 등 다양한 제품의 메모리 성능을 한층 끌어 올릴 수 있어 산업계에 파급력이 클 것으로 예상됩니다.\n",
    "특히, 연구팀은 이번 연구 결과를 적용하면 반도체 소형화시 저장 능력이 사라지는 문제점도 발생하지 않아 현재 10nm 수준에 멈춰 있는 반도체 공정을 0.5nm까지 미세화 할 수 있어 메모리 집적도가 기존 대비 약 1,000배 이상 향상될 것으로 예상했습니다.\n",
    "이준희 교수는 \"개별 원자에 정보를 저장하는 기술은 원자를 쪼개지 않는 범위 내에서 최고의 집적 기술\"이라며 \"이 기술을 활용하면 반도체 소형화가 더욱 가속화될 것으로 기대된다\"고 말했습니다.\n",
    "이번 연구는 2019년 12월 삼성미래기술육성사업 과제로 선정돼 연구 지원을 받고 있으며, 과학기술정보통신부 미래소재디스커버리 사업 지원도 받아 수행했습니다.\n",
    "한편 삼성미래기술육성사업은 국가 미래 과학기술 연구 지원을 위해 2013년부터 10년간 1조 5000억 원을 지원할 예정이며, 지금까지 589개 과제에 7589억 원의 연구비를 집행했습니다.\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#기사2\n",
    "sents = '''\n",
    "몇 차례에 걸쳐 개학이 연기되면서 우리 아이들의 학력을 걱정한 것이 지난봄의 일이었다.\n",
    "하지만 학교는 아주 짧은 시간 내에 ‘비대면 온라인 수업’과 ‘블렌디드 러닝(Blended Learning)’ 같은 새로운 교육환경을 학교 현장에 정착시켰다.\n",
    "이는 코로나19 이전부터 이미 AI와 빅데이터,IoT 기술 중심의 4차 산업혁명이 우리 사회와 교육에 역동적인 변화를 가져온 바탕이 있었기에 가능했고,학교교육 방법의 대전환을 가져오게 되었다.\n",
    "각 가정에서 온라인수업으로 학생이 스스로를 통제하며 모니터를 통해 선생님의 수업을 듣는 새로운 교육 환경이 지속된다면 우리가 아는 전통적인 교실수업은 사라지게 될 것이다.\n",
    "언젠가는 모든 학습활동이 비대면 학습활동으로 이뤄져 학교가 시·군 단위로 거점학교만 존재하고 선생님들이 많이 줄어들 수도 있다.\n",
    "학교의 경계가 무너지고 학년과 학급 중심의 교육과정이 무의미해지며,학생이 자기 현실에 맞게 교육과정을 주도적으로 결정하게 될 것이다.\n",
    "그렇다면 이러한 코로나19 이후 4차 산업혁명 시대 교육의 방향은 어떻게 설정해야 될까?우선 학생들이 자기 수준에 맞춰 언제 어디서든 학습이 가능하도록 ICT기반을 효율적으로 구축한 e스쿨의 체계적 학습관리 시스템이 조성되어야 한다.\n",
    "이를 위해 교사는 학습 테크놀로지 활용역량을 강화하고,웹사이트·포털·커뮤니티를 연결하는 에듀테크 기반 수업플랫폼을 구축해야 한다.\n",
    "학교는 학부모와 지역사회 단체와 다양한 네트워크를 강화해 교육의 주체인 학생,교직원,학부모,지역사회가 함께 교육과정을 운영하는 시스템을 만들어야 한다.\n",
    "교육환경이 변해도 결국 학습과 능력개발은 학습자 개인의 노력 여하에 따라 결정될 수밖에 없기 때문에 교실 밖에서 학생의 ‘자기주도적 학습능력’,문제해결 능력을 길러주는 것이 무엇보다 중요하다.\n",
    "여기에 교과 수업만이 아니라 학생 스스로 자신의 적성과 4차 산업혁명시대 새로운 직업군을 고려하여 진로를 선택하고 미래를 설계하는 ‘자기주도적 진로준비능력’까지 포함한 ‘자기주도적 역량’을 형성시켜야 한다.\n",
    "마지막으로 우리 아이들이 다양성을 존중하고 협력하면서 ‘공존’과 ‘상생’을 할 수 있게 공동체 의식을 함양시키는 ‘인성교육’을 게을리 하면 안 된다.\n",
    "시대 변화에 맞춰 디지털 리터러시와 생태환경과 같은 새로운 내용이 더해질 수는 있지만,지역사회와 함께 우리 아이들을 공정,평등,정의로운 사회의 일원으로 키워내는 시민교육의 실천은 모든 교육의 기본가치가 되어야 한다.\n",
    "이제 학교와 학교 밖,지역과 국경을 초월한 개방적 학교체제는 더 이상 미래교육이 아닌 현재 교육으로 다가왔다.\n",
    "이에 우리 교육은 공동체의 가치를 지키는 인성교육을 바탕으로 AI와 빅테이터 기반 에듀테크 교육 혁신을 통해 우리 학생들이 자기주도역량을 키우고 4차 산업혁명시대에 자신의 미래를 설계하는 능력을 갖추게 해야 한다.\n",
    "미래는 준비하는 자에게 길을 연다.그 준비의 시간은 바로 현재이다.\n",
    "'''\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#기사3\n",
    "sents = '''\n",
    "성남시의회는 최근 시의회 4층 세미나실에서 ‘사랑의 장기기증 운동 협약식’을 개최하며 생명 나눔 운동에 동참했다고 22일 밝혔다.\n",
    "시의회는 지난해 9월 성남시에서 태어나 어린 환자 3명에게 심장과 폐 등을 기증하고 짧은 생을 마감한 故서정민 군의 사연을 계기로 장기기증 운동에 솔선수범하여 동참하고, 시민들의 관심과 참여를 이끌고자 협약식을 마련했다.\n",
    "협약식에는 시의원들과 의회사무국 직원, 사랑의 장기기증운동본부 박진탁 이사장, 故서정민 군 부모 등이 참석했다.\n",
    "윤창근 의장과 박진탁 이사장이 생명 나눔 운동 협약을 체결했으며, 이후 윤창근 의장이 故서정민 군 부모에게 기림패를 전달했다.\n",
    "협약서 주요 내용은 ▶생명 나눔 운동 공동추진 ▶장기기증을 실천한 시민을 위한 ‘99공원(가칭)’조성 추진 ▶성남시의 장기기증 희망 등록자 및 실제 기증자 현황 연 1회 공유 ▶관내 기관 및 단체 구성원의 장기기증 희망등록 참여 독려 등이다.\n",
    "이후 시의원들은 장기기증운동본부 관계자로부터 장기기증의 필요성과 현황에 대한 설명을 들으며, 장기기증 희망 등록서 작성에 적극적으로 참여했다.\n",
    "시의원들은 의미 있는 일에 함께할 수 있어 기쁘다는 뜻을 밝혔다.\n",
    "윤창근 의장은 \"故서정민 군의 장기기증은 우리 사회에 큰 감동과 울림을 전해줬다.\n",
    "뇌사 시 장기기증으로 심장, 간장, 신장 2개, 폐장 2개, 췌장, 각막 2개 기증 등 9명의 생명을 구할 수 있다고 한다.\n",
    "장기기증은 소중한 생명을 구하는 값진 일임을 다시금 깨닫게 됐다\"며 \"장기기증을 기다리는 환우들의 아픔을 함께 나누며 장기기증에 대해 열린 마음을 갖는 사회가 되도록 성남시의회도 노력하겠다\"고 말했다.\n",
    "\n",
    "'''\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#기사4\n",
    "sents = '''\n",
    "미국 제약업체 화이자가 긴급사용승인을 신청한 가운데 신종 코로나바이러스 감염증(코로나19) 백신 개발 프로젝트 최고 책임자는 첫 접종이 내달 11일께 이뤄질 것으로 내다봤다.\n",
    "이에 따라 첫 접종이 시작되고 약 5개월 뒤에는 집단면역도 가능할 것이라는 전망이다.\n",
    "몬세프 슬라위 '워프스피드 작전'(operation warp speed) 최고책임자는 22일(현지시간) CNN과의 \"인구의 70% 정도가 면역력을 갖는다면 집단면역이 일어날 수 있을 것\"이라며 \"우리 계획에 따르면 5월쯤 그런 일이 일어날 것 같다\"고 말했다.\n",
    "슬라위 최고책임자에 따르면 화이자와 독일 바이오엔테크가 공동 개발한 코로나19 백신은 이르면 내달 11일부터 미국에서 접종이 시작된다.\n",
    "내달 10일 미 식품의약국(FDA)은 자문위원회 회의를 열고 화이자의 긴급사용 신청 승인을 논의할 예정이다.\n",
    "그는 \"승인으로부터 24시간 안에 백신을 접종 장소로 실어나르는 것이 우리의 계획\"이라며 \"승인 다음 날인 12월 11일이나 12일 첫 번째 사람들이 미국 전역에서 접종을 받을 수 있을 것으로 기대한다\"고 설명했다.\n",
    "슬라위 최고책임자에 따르면 긴급사용 승인 신청 논의와 승인, 백신 수송 등 일정이 예상대로 진행될 경우 12월에는 최대 2000만명이 접종을 받게 된다.\n",
    "이후에는 매달 3000만명이 백신을 접종받을 것으로 관측된다.\n",
    "'''\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#기사5\n",
    "sents = '''\n",
    "코스피 사상 최고 행진을 이끈 것은 '외국인 선호'가 몰린 분야인 반도체·배터리·철강·자동차 부품 부문이었다.\n",
    "올해 3분기(7~9월)까지는 '동학개미 열풍' 속에 이른바 'BBIG(바이오·배터리·인터넷·게임) 종목이 코스피를 끌어왔지만 최근 들어서는 '수출 한국'을 대표하는 이들 종목이 외국인 매수에 힘입어 지수를 끌어올렸다.\n",
    "코스피가 사상 최고치(2018년 1월 29일·2598.19)를 찍던 당시 증시를 이끌던 상위 1~10위 종목은 지금과 비슷하지만 글로벌 경기 회복세 속에 각국 중앙은행이 기준금리를 올리던 당시와 달리 올해는 코로나19 사태에 따른 실물경제 타격 속에 각국이 비정상적인 무제한 돈 풀기에 나섰다는 점이 가장 큰 차이다.\n",
    "다만 코로나19 백신이 내년에 본격적으로 접종되기 시작하면 글로벌 자금이 한국 등 신흥 시장에서 미국 유럽 등 주요국 증시로 옮아갈 것이라는 예상이 나온다.\n",
    "23일 증시에서는 코스피 기준 시가총액 1~2위를 책임지는 반도체 분야 약진이 두드러졌다.\n",
    "'코스피 시총 1위'로 한국 간판 주식으로 통하는 삼성전자가 전날보다 4.33% 오른 급등세를 보이며 역대 최고가 기록을 갈아치운 가운데 SK하이닉스(3.31%), DB하이텍(2.21%) 등 반도체 3대장 다른 종목들도 상승세를 보였다.\n",
    "특히 삼성전자는 반도체 업황 턴어라운드에 따른 실적 개선 기대감 외에 투자자들의 배당 확대 기대감이 더해진 결과 삼성전자 우선주 주가도 4.64% 올라 눈길을 끌었다.\n",
    "전기자동차(EV) 등 친환경 성장 산업과 관련한 배터리 분야 약진도 두드러졌다.\n",
    "배터리 3총사로 꼽히는 SK이노베이션은 LG화학과의 분쟁 등 여파로 제자리걸음했지만 대장주인 LG화학 주가가 3.31% 올랐고, 삼성SDI도 덩달아 2.14% 상승했다.\n",
    "지난 15일 각국 정상 서명식을 통해 관심을 받은 역내포괄적경제동반자협정(RCEP) 관련 종목도 주가가 빠르게 올랐다.\n",
    "철강 3형제로 꼽히는 포스코(2.53%)와 현대제철(4.08%), 동국제강(13.98%) 시세가 가파르게 올랐다.\n",
    "자동차 부품 수혜주로 꼽히는 만도도 하루 새 3.39% 뛰었다.\n",
    "다만 이들 종목은 단순히 RCEP 수혜주라는 점 외에도 실물경제가 회복했을 때 철강·자동차 산업 수요가 이어질 것이라는 점, 현대제철과 만도 등은 각각 수소차·전기차 성장 산업과도 연결돼 있다는 점에서도 투자자들의 손길이 몰렸다.\n",
    "한편에서는 지난 9월 이후 고전했던 BBIG 종목 성장주가 다시 오름세를 보일 것이라는 기대감도 나온다.\n",
    "문종진 교보증권 연구원은 \"10월 이후 종목 주가 상승 부담과 더불어 백신 개발 기대, 연말 배당 시즌 등으로 투자자들이 성장주보다 가치주를 선호하는 현상이 나타나는 분위기\"라면서도 \"BBIG 주요 기업 이익 등 실적 전망치가 꾸준히 높아지고 있어 지금 같은 추세라면 다시 상승세가 찾아올 가능성이 크다\"고 평가했다.\n",
    "이날 한국거래소(KRX)와 에프앤가이드 등에 따르면 'KRX BBIG K-뉴딜지수' 12개 구성 종목의 올해 3분기 연결기준 영업이익이 지난해 같은 기간보다 45.5% 늘어난 2조1529억원으로 집계돼 성장세를 과시했다.\n",
    "거래소가 집계한 코스피·코스닥 전체 상장사 1548곳(금융업 등 제외)의 같은 기간 영업이익 증가율(22.7%)의 2배다.\n",
    "2차전지(LG화학·삼성SDI·SK이노베이션)와 바이오(삼성바이오로직스·셀트리온·SK바이오팜), 인터넷(네이버·카카오·더존비즈온), 게임(엔씨소프트·넷마블·펄어비스) 등이 해당 종목이다.\n",
    "\n",
    "'''\n"
   ]
  },
  "source": [
    "모델 1 4 구현에 있어 필요한 함수들 구현/ 모델 2 3은 라이브러리 이용"
   ]
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9nPczqkwBraJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
    "\n",
    "    assert 0 < df < 1 \n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    if bias is None:\n",
    "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "    else:\n",
    "        bias = bias.reshape(-1,1)\n",
    "        bias = A.shape[0] * bias / bias.sum()\n",
    "        assert bias.shape[0] == A.shape[0]\n",
    "        bias = (1 - df) * bias\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "    return R\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# 문장의 유사도에 따라 edge를 연결하는 그래프 생성\n",
    "def sent_graph(sents, tokenize=None, min_count=2, min_sim=0.3,\n",
    "    similarity=None, vocab_to_idx=None, verbose=False):\n",
    "    if vocab_to_idx is None:\n",
    "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    else:\n",
    "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
    "    x = vectorize_sents(sents, tokenize, vocab_to_idx)\n",
    "    if similarity == 'cosine':\n",
    "        x = numpy_cosine_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    else:\n",
    "        x = numpy_textrank_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    return x\n",
    "\n",
    "# 데이터를 벡터로 표현\n",
    "def vectorize_sents(sents, tokenize, vocab_to_idx):\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, sent in enumerate(sents):\n",
    "        counter = Counter(tokenize(sent))\n",
    "        for token, count in counter.items():\n",
    "            j = vocab_to_idx.get(token, -1)\n",
    "            if j == -1:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(count)\n",
    "    n_rows = len(sents)\n",
    "    n_cols = len(vocab_to_idx)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "# Cosine 유사도로 문장 간 edge 연결 결정\n",
    "def numpy_cosine_similarity_matrix(x, min_sim=0.3, verbose=True, batch_size=1000):\n",
    "    n_rows = x.shape[0]\n",
    "    mat = []\n",
    "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
    "        b = int(bidx * batch_size)\n",
    "        e = min(n_rows, int((bidx+1) * batch_size))\n",
    "        psim = 1 - pairwise_distances(x[b:e], x, metric='cosine')\n",
    "        rows, cols = np.where(psim >= min_sim)\n",
    "        data = psim[rows, cols]\n",
    "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
    "        if verbose:\n",
    "            print('\\rcalculating cosine sentence similarity {} / {}'.format(b, n_rows), end='')\n",
    "    mat = sp.sparse.vstack(mat)\n",
    "    if verbose:\n",
    "        print('\\rcalculating cosine sentence similarity was done with {} sents'.format(n_rows))\n",
    "    return mat\n",
    "\n",
    "def numpy_textrank_similarity_matrix(x, min_sim=0.3, verbose=True, min_length=1, batch_size=1000):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    rows, cols = x.nonzero()\n",
    "    data = np.ones(rows.shape[0])\n",
    "    z = csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "    size = np.asarray(x.sum(axis=1)).reshape(-1)\n",
    "    size[np.where(size <= min_length)] = 10000\n",
    "    size = np.log(size)\n",
    "\n",
    "    mat = []\n",
    "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
    "\n",
    "        # slicing\n",
    "        b = int(bidx * batch_size)\n",
    "        e = min(n_rows, int((bidx+1) * batch_size))\n",
    "\n",
    "        # dot product\n",
    "        inner = z[b:e,:] * z.transpose()\n",
    "\n",
    "        # sentence len[i,j] = size[i] + size[j]\n",
    "        norm = size[b:e].reshape(-1,1) + size.reshape(1,-1)\n",
    "        norm = norm ** (-1)\n",
    "        norm[np.where(norm == np.inf)] = 0\n",
    "\n",
    "        # normalize\n",
    "        sim = inner.multiply(norm).tocsr()\n",
    "        rows, cols = (sim >= min_sim).nonzero()\n",
    "        data = np.asarray(sim[rows, cols]).reshape(-1)\n",
    "\n",
    "        # append\n",
    "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
    "\n",
    "        if verbose:\n",
    "            print('\\rcalculating textrank sentence similarity {} / {}'.format(b, n_rows), end='')\n",
    "\n",
    "    mat = sp.sparse.vstack(mat)\n",
    "    if verbose:\n",
    "        print('\\rcalculating textrank sentence similarity was done with {} sents'.format(n_rows))\n",
    "\n",
    "    return mat\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class KeysentenceSummarizer:\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.min_sim = min_sim\n",
    "        self.similarity = similarity\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "    \n",
    "    # 문장을 그래프로 구현한 후 pagerank로 유사도 순위 계산\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
    "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
    "    \n",
    "    # textrank를 이용해 요약한 문서 리스트를 반환\n",
    "    def summarize(self, sents, topk=30, bias=None):\n",
    "        n_sents = len(sents)\n",
    "        if isinstance(bias, np.ndarray):\n",
    "            if bias.shape != (n_sents,):\n",
    "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
    "        elif bias is not None:\n",
    "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
    "\n",
    "        self.train_textrank(sents, bias)\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keysents = [sents[idx] for idx in reversed(idxs)]\n",
    "        return keysents\n",
    "\n",
    "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "\n",
    "    ##2\n",
    "    ##print(\"단어 인덱스\")\n",
    "    ##print(idx_to_vocab)\n",
    "    ##print(vocab_to_idx)\n",
    "    ##본문 단어의 인덱스화\n",
    "\n",
    "    return idx_to_vocab, vocab_to_idx\n",
    "\n",
    "sents_arr = sents.split(\".\")\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "def komoran_tokenizer(sent):\n",
    "    ##print(\"한국어 형태소 분석기\")\n",
    "    # 형태소 분석기로 나눈 단어에 품사 부착\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    ##print(words)\n",
    "    ## 문법 기능의 단어 제거\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    ##print(words)\n",
    "    return words\n",
    "    \n",
    "##print('cosine or textrank 선택')\n",
    "##['cosine', 'textrank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lOsOmO1G1C2s"
   },
   "outputs": [],
   "source": [
    "import networkx\n",
    "import re\n",
    "\n",
    "class RawSentence:\n",
    "    def __init__(self, textIter):\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    "                \n",
    "class TextRank:\n",
    "    def __init__(self, **kargs):\n",
    "        self.graph = None\n",
    "        self.window = kargs.get('window', 5)\n",
    "        self.coef = kargs.get('coef', 1.0)\n",
    "        self.threshold = kargs.get('threshold', 0.005)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "        self.dictNear = {}\n",
    "        self.nTotal = 0\n",
    "\n",
    "    def loadSents(self, sentenceIter, tokenizer = None):\n",
    "        import math\n",
    "        def similarity(a, b):\n",
    "            n = len(a.intersection(b))\n",
    "            return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
    " \n",
    "        if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
    "        sentSet = []\n",
    "        \n",
    "        for sent in filter(None, sentenceIter):\n",
    "            if type(sent) == str:\n",
    "                if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
    "                else: s = set(filter(None, rgxSplitter.split(sent)))\n",
    "                    \n",
    "            else: s = set(sent)\n",
    "            if len(s) < 2: continue\n",
    "            self.dictCount[len(self.dictCount)] = sent\n",
    "            sentSet.append(s)\n",
    " \n",
    "        for i in range(len(self.dictCount)):\n",
    "            for j in range(i+1, len(self.dictCount)):\n",
    "                s = similarity(sentSet[i], sentSet[j])\n",
    "                if s < self.threshold: continue\n",
    "                self.dictBiCount[i, j] = s\n",
    "    \n",
    "    def build(self):\n",
    "        self.graph = networkx.Graph()\n",
    "        self.graph.add_nodes_from(self.dictCount.keys())\n",
    "        for (a, b), n in self.dictBiCount.items():\n",
    "            self.graph.add_edge(a, b, weight=n*self.coef + (1-self.coef))\n",
    " \n",
    "    def rank(self):\n",
    "        return networkx.pagerank(self.graph, weight='weight')\n",
    " \n",
    "    def summarize(self, lines = 3):\n",
    "        r = self.rank()\n",
    "        ks = sorted(r, key=r.get, reverse=True)[:lines]\n",
    "        arr = [] \n",
    "        top = sorted(ks)\n",
    "        for i in range(lines):\n",
    "            arr.append(self.dictCount[top[i]])\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffH345aqQMeU"
   },
   "source": [
    "4가지 MODEL"
   ]
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WJFrxVGVNqbw"
   },
   "outputs": [],
   "source": [
    "def model_1(sent):\n",
    "    summarizer = KeysentenceSummarizer(tokenize = komoran_tokenizer,similarity='textrank', min_sim = 0.3)\n",
    "    keysents = summarizer.summarize(sent, topk=5)\n",
    "    return keysents"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Vir2krUNNQzi"
   },
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "def model_2(sent):\n",
    "  keysents = summarize(sent, word_count=100, split=True)\n",
    "  return keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2zK4D9HJNuBt"
   },
   "outputs": [],
   "source": [
    "from lexrankr import LexRank\n",
    "\n",
    "def model_3(sent):\n",
    "  lexrank = LexRank()\n",
    "  lexrank.summarize(sent)\n",
    "  keysents = lexrank.probe(5)\n",
    "  return keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Zd7l2QMbONnW"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "def model_4(sent):\n",
    "  tagger = Komoran()\n",
    "  tr = TextRank()\n",
    "  stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV') ])\n",
    "  tr.loadSents(RawSentence(sent), lambda sent: filter(lambda x:x not in stopword and x[1] in ('NNG', 'NNP', 'VV', 'VA'), tagger.pos(sent)))\n",
    "  tr.build()\n",
    "  keysents = tr.summarize(5)\n",
    "  return keysents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHx2OEmm1C28"
   },
   "source": [
    "Print each algorithms result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iJ_nSgTfOsfm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['\\n다만 이들 종목은 단순히 RCEP 수혜주라는 점 외에도 실물경제가 회복했을 때 철강·자동차 산업 수요가 이어질 것이라는 점, 현대제철과 만도 등은 각각 수소차·전기차 성장 산업과도 연결돼 있다는 점에서도 투자자들의 손길이 몰렸다', '\\n문종진 교보증권 연구원은 \"10월 이후 종목 주가 상승 부담과 더불어 백신 개발 기대, 연말 배당 시즌 등으로 투자자들이 성장주보다 가치주를 선호하는 현상이 나타나는 분위기\"라면서도 \"BBIG 주요 기업 이익 등 실적 전망치가 꾸준히 높아지고 있어 지금 같은 추세라면 다시 상승세가 찾아올 가능성이 크다\"고 평가했다', '19)를 찍던 당시 증시를 이끌던 상위 1~10위 종목은 지금과 비슷하지만 글로벌 경기 회복세 속에 각국 중앙은행이 기준금리를 올리던 당시와 달리 올해는 코로나19 사태에 따른 실물경제 타격 속에 각국이 비정상적인 무제한 돈 풀기에 나섰다는 점이 가장 큰 차이다', \"\\n올해 3분기(7~9월)까지는 '동학개미 열풍' 속에 이른바 'BBIG(바이오·배터리·인터넷·게임) 종목이 코스피를 끌어왔지만 최근 들어서는 '수출 한국'을 대표하는 이들 종목이 외국인 매수에 힘입어 지수를 끌어올렸다\", \"\\n코스피 사상 최고 행진을 이끈 것은 '외국인 선호'가 몰린 분야인 반도체·배터리·철강·자동차 부품 부문이었다\"]\n",
      "2\n",
      "['다만 코로나19 백신이 내년에 본격적으로 접종되기 시작하면 글로벌 자금이 한국 등 신흥 시장에서 미국 유럽 등 주요국 증시로 옮아갈 것이라는 예상이 나온다.', '23일 증시에서는 코스피 기준 시가총액 1~2위를 책임지는 반도체 분야 약진이 두드러졌다.', '배터리 3총사로 꼽히는 SK이노베이션은 LG화학과의 분쟁 등 여파로 제자리걸음했지만 대장주인 LG화학 주가가 3.31% 올랐고, 삼성SDI도 덩달아 2.14% 상승했다.', '철강 3형제로 꼽히는 포스코(2.53%)와 현대제철(4.08%), 동국제강(13.98%) 시세가 가파르게 올랐다.', '다만 이들 종목은 단순히 RCEP 수혜주라는 점 외에도 실물경제가 회복했을 때 철강·자동차 산업 수요가 이어질 것이라는 점, 현대제철과 만도 등은 각각 수소차·전기차 성장 산업과도 연결돼 있다는 점에서도 투자자들의 손길이 몰렸다.', '한편에서는 지난 9월 이후 고전했던 BBIG 종목 성장주가 다시 오름세를 보일 것이라는 기대감도 나온다.']\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KimSJ\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코스피가 사상 최고치(2018년 1월 29일·2598.19)를 찍던 당시 증시를 이끌던 상위 1~10위 종목은 지금과 비슷하지만 글로벌 경기 회복세 속에 각국 중앙은행이 기준금리를 올리던 당시와 달리 올해는 코로나19 사태에 따른 실물경제 타격 속에 각국이 비정상적인 무제한 돈 풀기에 나섰다는 점이 가장 큰 차이다', \"'코스피 시총 1위'로 한국 간판 주식으로 통하는 삼성전자가 전날보다 4.33% 오른 급등세를 보이며 역대 최고가 기록을 갈아치운 가운데 SK하이닉스(3.31%), DB하이텍(2.21%) 등 반도체 3대장 다른 종목들도 상승세를 보였다\", '다만 이들 종목은 단순히 RCEP 수혜주라는 점 외에도 실물경제가 회복했을 때 철강·자동차 산업 수요가 이어질 것이라는 점, 현대제철과 만도 등은 각각 수소차·전기차 성장 산업과도 연결돼 있다는 점에서도 투자자들의 손길이 몰렸다', '한편에서는 지난 9월 이후 고전했던 BBIG 종목 성장주가 다시 오름세를 보일 것이라는 기대감도 나온다', \"이날 한국거래소(KRX)와 에프앤가이드 등에 따르면 'KRX BBIG K-뉴딜지수' 12개 구성 종목의 올해 3분기 연결기준 영업이익이 지난해 같은 기간보다 45.5% 늘어난 2조1529억원으로 집계돼 성장세를 과시했다\"]\n",
      "4\n",
      "[\"코스피 사상 최고 행진을 이끈 것은 '외국인 선호'가 몰린 분야인 반도체·배터리·철강·자동차 부품 부문이었다.\", '23일 증시에서는 코스피 기준 시가총액 1~2위를 책임지는 반도체 분야 약진이 두드러졌다.', \"'코스피 시총 1위'로 한국 간판 주식으로 통하는 삼성전자가 전날보다 4.33% 오른 급등세를 보이며 역대 최고가 기록을 갈아치운 가운데 SK하이닉스(3.31%), DB하이텍(2.21%) 등 반도체 3대장 다른 종목들도 상승세를 보였다.\", '전기자동차(EV) 등 친환경 성장 산업과 관련한 배터리 분야 약진도 두드러졌다.', '한편에서는 지난 9월 이후 고전했던 BBIG 종목 성장주가 다시 오름세를 보일 것이라는 기대감도 나온다.']\n"
     ]
    }
   ],
   "source": [
    "print(\"1\")\n",
    "print(model_1(sents_arr))\n",
    "\n",
    "print(\"2\")\n",
    "print(model_2(sents))\n",
    "\n",
    "print(\"3\")\n",
    "print(model_3(sents))\n",
    "\n",
    "print(\"4\")\n",
    "print(model_4(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rUruytk1C3B"
   },
   "source": [
    "Combine the results and make new result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "h3410zq7RuCM"
   },
   "outputs": [],
   "source": [
    "def Koreansent(before):\n",
    "    after = \"\"\n",
    "    for c in before:\n",
    "        if ord('가') <= ord(c) <= ord('힣'):\n",
    "            after = after + c\n",
    "    return after\n",
    "\n",
    "def samesent(a,b):\n",
    "    text1 = Koreansent(a)\n",
    "    text2 = Koreansent(b)\n",
    "    if(text1==text2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def mixlist(lista , listb):\n",
    "    lenb = len(listb)\n",
    "    lena = len(lista)\n",
    "    n=0\n",
    "    for i in range(lenb):\n",
    "        for j in range(lena):\n",
    "            if(samesent(lista[j],listb[i])):\n",
    "                n=1\n",
    "            else:\n",
    "                continue\n",
    "        if(n==0):\n",
    "            lista.append(listb[i])\n",
    "        n=0\n",
    "    return lista\n",
    "\n",
    "def mcountlist(mainlist , sublist,countlist):\n",
    "    mainlen = len(mainlist)\n",
    "    sublen = len(sublist)\n",
    "    n=0\n",
    "    for i in range(mainlen):\n",
    "        for j in range(sublen):\n",
    "            if(samesent(mainlist[i],sublist[j])):\n",
    "               n=1\n",
    "            else:\n",
    "                continue\n",
    "        if(n==1):\n",
    "            countlist[i] = countlist[i] +1\n",
    "        n=0\n",
    "    return countlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6eqJN-CPDcC"
   },
   "source": [
    "최종모델 ( 1번 2번 3번 4번 모델 결합 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KimSJ\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "코스피 사상 최고 행진을 이끈 것은 '외국인 선호'가 몰린 분야인 반도체·배터리·철강·자동차 부품 부문이었다\n",
      "23일 증시에서는 코스피 기준 시가총액 1~2위를 책임지는 반도체 분야 약진이 두드러졌다.\n",
      "\n",
      "다만 이들 종목은 단순히 RCEP 수혜주라는 점 외에도 실물경제가 회복했을 때 철강·자동차 산업 수요가 이어질 것이라는 점, 현대제철과 만도 등은 각각 수소차·전기차 성장 산업과도 연결돼 있다는 점에서도 투자자들의 손길이 몰렸다\n",
      "한편에서는 지난 9월 이후 고전했던 BBIG 종목 성장주가 다시 오름세를 보일 것이라는 기대감도 나온다.\n"
     ]
    }
   ],
   "source": [
    "def Finalmodel1_ver2():\n",
    "    keysent1 = model_1(sents_arr)\n",
    "    keysent2 = model_2(sents)\n",
    "    keysent3 = model_3(sents)\n",
    "    keysent4 = model_4(sents)\n",
    "    Mixlist=mixlist(mixlist(mixlist(keysent1,keysent2),keysent3),keysent4)\n",
    "    mixlen = len(Mixlist)\n",
    "    countlist = [0]*mixlen\n",
    "    countlist=mcountlist(Mixlist,keysent1,countlist)\n",
    "    countlist=mcountlist(Mixlist,keysent2,countlist)\n",
    "    countlist=mcountlist(Mixlist,keysent3,countlist)\n",
    "    countlist=mcountlist(Mixlist,keysent4,countlist)\n",
    "\n",
    "    maxnum = max(countlist)\n",
    "    rcountlist = [0]*mixlen\n",
    "    for i in range(mixlen):\n",
    "        rcountlist[i] = maxnum - countlist[i] + 1\n",
    "\n",
    "    from queue import PriorityQueue\n",
    "    que = PriorityQueue()\n",
    "    for i in range(mixlen):\n",
    "        que.put((rcountlist[i],Mixlist[i]))\n",
    "\n",
    "    sents_n = 5 #추출할 문장 갯수\n",
    "    sents_list_unorder = []\n",
    "    for i in range(sents_n):\n",
    "        sents_list_unorder.append(que.get()[1])\n",
    "\n",
    "    sents_list_order = []\n",
    "    for i in range(len(sents_arr)):\n",
    "        for j in range(sents_n):\n",
    "            if(samesent(sents_arr[i],sents_list_unorder[j])):\n",
    "                sents_list_order.append(sents_list_unorder[j])\n",
    "\n",
    "    for i in range(len(sents_list_order)):\n",
    "        print(sents_list_order[i])\n",
    "    \n",
    "Finalmodel1_ver2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "비지도모델(ver.한국어)_미완성본.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
